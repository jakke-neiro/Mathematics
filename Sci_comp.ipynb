{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\section{Derivatives and optimisation in Python}\n",
    "Calculating derivatives: symbolic, numerical, automatic\n",
    "\n",
    "Why calculate derivatives in optimisation? Assume the function $f(x)$ you wish to optimise is differentiable, then we can consider the equation of a tangen line at a given point $x_n$\n",
    "\\[\n",
    "y(x) = f(x_n) + f'(x_n)(x - x_n)\n",
    "\\]\n",
    "We want to find the value of $x_{n+1}$ such that $y(x_{n+1}) = 0$, i.e.\n",
    "\\[\n",
    "0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)\n",
    "\\]\n",
    "Rearrange to get:\n",
    "\\[\n",
    "x_{n+1} = x + n - \\frac{f(x_n)}{f'(x_n)}\n",
    "\\]\n",
    "This is Newton's method for root finding. It's easy to extend to minimisation/maximisation of $f(x)$. We know that local minima/maxima exist when $f'(x) = 0$, so one can apply Newton's method to the derivative instead:\n",
    "\\[\n",
    "x_{n+1} = x + n - \\frac{f'(x_n)}{f''(x_n)}\n",
    "\\]\n",
    "Several important optimisation methods use gradient information to find the minima/maxima of a differentiable function: gradient descent, conjugate gradient descent, Newton's, Quasi-Newton such as Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
    "\n",
    "\\subsection{Numerical differentiation}\n",
    "Taylor series expansion:\n",
    "\\[\n",
    "f(x_0 + h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\dots\n",
    "\\]\n",
    "\\[\n",
    "f(x_0 - h) = f(x_0) - hf'(x_0) + \\frac{h^2}{2!}f''(x_0) - \\frac{h^3}{3!}f'''(x_0) +\n",
    "\\]\n",
    "This leads to \n",
    "\n",
    "Forward difference: \n",
    "\\[\n",
    "f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} + O(h)\n",
    "\\]\n",
    "Backwards difference\n",
    "\\[\n",
    "f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} + O(h)\n",
    "\\]\n",
    "Central difference\n",
    "\\[\n",
    "f'(x_0) = \\frac{f(x_0 + h) - f(x_0 -h)}{2h} + O(h^2)\n",
    "\\]\n",
    "Automatic differentiation is the application of the chain rule to a computer program\n",
    "\n",
    "Imagine a Pythonb function with an input $x$ and result $y$. Statements in this function might consist of:\n",
    "\\[\n",
    "w_0 = x\n",
    "\\]\n",
    "\\[\n",
    "w_1 = h(w_0)\n",
    "\\]\n",
    "\\[\n",
    "w_2 = g(w_1)\n",
    "\\]\n",
    "\\[\n",
    "w_3 = f(w_2)\n",
    "\\]\n",
    "\\[\n",
    "y = w_3\n",
    "\\]\n",
    "This is the same as\n",
    "\\[\n",
    "y = f(g(h(x)))\n",
    "\\]\n",
    "And the chain rule gives\n",
    "\\[\n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1}\\frac{\\partial w_1}{\\partial w_0}\\frac{\\partial w_0}{\\partial x}\n",
    "\\]\n",
    "Froward mode and reverse mode AD\n",
    "\n",
    "Local minimisation\n",
    "\n",
    "Simplex methods: based on evolving simplex, or geometrical volume, through subsequent iterations. No gradients required. Nelder-Mead\n",
    "\n",
    "Gradient-based methods: gradient descent, conjugate gradients,\n",
    "\n",
    "Root-finding methods: Nweton's, Broyden-Fletcher-Goldfarb-Shanno\n",
    "\n",
    "\\section{Model fitting}\n",
    "Polynomial model and ordinary differential equation model\n",
    "\n",
    "Your model for the function $y(x)$ is given by a polynomial of degree $n$\n",
    "\\[\n",
    "y(x) = \\sum_{k=0}^n \\beta_k x^k\n",
    "\\]\n",
    "For a dataset $y, x$ of size $m$, this gives a set of $m$ equations\n",
    "\\[\n",
    "y_j = \\sum_{k=0}^n \\beta_k x_j^k, j=0, \\dots, m-1\n",
    "\\]\n",
    "Can cobmine into a single linear algebra equation:\n",
    "\\[\n",
    "y = M\\beta\n",
    "\\]\n",
    "where $M$ is an $m \\times n$ matrix with entries $M_{j,k} = x_j^k$\n",
    "\n",
    "\\subsection{Solving polynomial regression}\n",
    "We want to find the vector $\\beta$ such that $\\beta = \\textrm{arg min}_{\\beta}\\| y - M\\beta \\|$, can do this by using ordinary least squares\n",
    "\\[\n",
    "\\beta = (M^TM)^{-1}M^T y\n",
    "\\]\n",
    "Ordinary least square can suffer for ill-conditioning for large number of parameters, so we can add small positive elements to the diagonal to improve the reliability of the method (Tikhonov or ridge regression)\n",
    "\\[\n",
    "\\beta = (M^T M + \\lambda I)^{-1}M^Ty\n",
    "\\]\n",
    "Can evaluate the goodness of fit of the model to data via\n",
    "\\begin{enumerate}\n",
    "    \\item Residuals: $|y - \\beta M|$. This gives a poor evaluation of how good the model is at prediction to new data points.\n",
    "    \\item Cross-validation: more direct measure of the predictive power of the model. We divide the model into a training set, and a test set. Us ethe training set to calculate $\\beta$, and then evaluate the model at the positions of the test set and measure the error\n",
    "    \\item k-fold cross-validation and leave-one-out cross-validation\n",
    "\\end{enumerate}\n",
    "\n",
    "\\subsection{Fitting ODE models}\n",
    "Our model is now:\n",
    "\\[\n",
    "\\frac{dy}{dx} = f(x, y, \\beta)\n",
    "\\]\n",
    "More difficult to evaluate than a polynomial model. Simplest approach is to assume that the data has been sampled from a normal distribution with the mean given by the solution of the ODE:\n",
    "\\[\n",
    "z_j \\sim y(x_j, \\beta) + N(0, \\sigma)\n",
    "\\]\n",
    "We can calculate a likelihood of a given set of parameters $\\beta$, given the data $y$:\n",
    "\\[\n",
    "L(\\beta|z) = \\prod_j^m \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\bigg( - \\frac{(z_j - y(x_j, \\beta))^2}{2 \\sigma^2}\n",
    "\\]\n",
    "Normally work with the log-likelihood:\n",
    "\\[\n",
    "\\log L(\\beta|z) = - \\frac{m}{2} \\log(2 \\pi) - m \\log(\\sigma) - \\frac{1}{2 \\sigma^2} \\sum_j^m (z_j - y(x_j, \\beta))^2\n",
    "\\]\n",
    "We want to find the parameters $\\beta$ that maximise the log-likelihood. Assuming $m$ and $\\sigma$ are fixed, this corresponds to minimising the sum-of-squares of the residuals\n",
    "\\[\n",
    "\\sum_j^m (z_j - y(x_j, \\beta))^2\n",
    "\\]\n",
    "\n",
    "\\section{Finite difference solution of PDEs}\n",
    "Based on taylor series. Taylor expand $u(x+h)$ and $u(x-h)$ (and others) in small parameter $h$ about $x$. The Taylor expansion of a smooth function $u(x)$ is\n",
    "\\[\n",
    "u(x+h) = u(x) + hu'(x) + \\frac{h^2}{2}u''(x) + \\frac{h^3}{6}u'''(x) + \\frac{h^4}{24}u''''(x) + \\dots\n",
    "\\]\n",
    "\\[\n",
    "u(x-h) = u(x) - hu'(x) + \\frac{h^2}{2}u''(x) - \\frac{h^3}{6}u'''(x) + \\frac{h^4}{24}u''''(x) - \\dots\n",
    "\\]\n",
    "\n",
    "we can compute three difference schemes (approximations) to $u'(x)$\n",
    "\n",
    "Forward difference\n",
    "\\[\n",
    "u'(x) = \\frac{u(x+h) - u(x)}{h} + O(h)\n",
    "\\]\n",
    "Backward difference\n",
    "\\[\n",
    "u'(x) = \\frac{u(x) - u(x-h)}{h} + O(h)\n",
    "\\]\n",
    "Central difference\n",
    "\\[\n",
    "u'(x) = \\frac{u(x+h) - u(x-h)}{2h} + O(h^2)\n",
    "\\]\n",
    "\n",
    "Notation:\n",
    "\\begin{enumerate}\n",
    "    \\item Partial derivatives $u_t = \\frac{\\partial u}{\\partial t}$, $u_{xx} = \\frac{\\partial^2 u}{\\partial x}$\n",
    "    \\item Gradient (a vector) $\\nabla u = (u_x, u_y, u_z)$\n",
    "    \\item Laplacian (a scalar) $\\nabla^2 u = \\Delta u = u_{xx} + u_{yy} + u_{zz}$\n",
    "    \\item Divergence (vector input, scalar output) $\\nabla \\cdot (u, v, w)^T = u_x + u_y + w_z$\n",
    "\\end{enumerate}\n",
    "\n",
    "Laplace equation: $\\Delta u = 0$ (elliptic)\n",
    "\n",
    "Poisson equation: $\\Delta u = f(x, y, z)$ (elliptic)\n",
    "\n",
    "Heat or diffusion equation: $u_t = \\Delta u$ (parabolic)\n",
    "\n",
    "Wave equation: $u_{tt} = \\Delta u$ (hyperbolic)\n",
    "\n",
    "Burgers equation: $u_t = (u^2)_x + \\epsilon u_{xx}$\n",
    "\n",
    "KdV equation: $u_t = (u^2)_x + u_{xxx}$\n",
    "\n",
    "\\subsection{FD approximation of Laplace equation}\n",
    "If we have the 1D Laplace equation\n",
    "\\[\n",
    "u_{xx} = 0 , 0 \\leq x \\leq 1\n",
    "\\]\n",
    "then the central FD approximation of $u_{xx}$ is:\n",
    "\\[\n",
    "u_{xx} \\approx \\frac{u(x+h) - 2u(x) + u(x-h)}{h^2}\n",
    "\\]\n",
    "\n",
    "Discretise $u_{xx} = 0$ in space. Gives a set of $N$ equations at each interior point on the domain:\n",
    "\\[\n",
    "\\frac{v_{i+1} - 2v_i + v_{i-1}}{h^2} = 0, i=1, \\dots, N\n",
    "\\]\n",
    "where $v_i \\approx u(x_i)$.\n",
    "\n",
    "\\subsection{Boundary conditions}\n",
    "We need boundary conditions to solve these equations. Boundary conditions normally one of two types:\n",
    "\\begin{enumerate}\n",
    "    \\item Dirichlet: a condition on the value of $u$, e.g. $u(0) = 0$\n",
    "    \\item Neumann: a condition on the derivative of $u$, e.g. $u_x(0) = 0$\n",
    "\\end{enumerate}\n",
    "In this case, the boundary conditions act as equations at $x_0$ and $x_{N+1}$. \n",
    "\n",
    "\\subsubsection{Homogenous Dirichlet boundary conditions}\n",
    "Take for example $u(0) = 0$ (homogenous dirichlet bc), this gives $v_0 = 0$, and the equation at $x_1$ becomes:\n",
    "\\[\n",
    "\\frac{v_{i+1} - 2v_i + 0}{h^2} = 0\n",
    "\\]\n",
    "Lets put a similar bc at the other boundary, $u(1) = 0$, and represent the final $N$ equations in matrix format:\n",
    "\\[\n",
    "\\frac{1}{h^2}\n",
    "\\begin{bmatrix}\n",
    "-2 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddpots & \\vdots \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{N-1} \\\\ v_N \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\subsection{Non-homogenous Dirichlet boundary conditions}\n",
    "Assuming that we have the following bcs:\n",
    "$u(0) = 1, u(1) = 0$, then \n",
    "\\[\n",
    "\\frac{1}{h^2}\n",
    "\\begin{bmatrix}\n",
    "-2 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddpots & \\vdots \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{N-1} \\\\ v_N \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\frac{1}{h^2}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Neumann boundary conditions}\n",
    "Assume that we have the following boundary conditions: $u_x(0) = 0, u(1) = 0$. The boundary condition at $x = 0$ gives the following equation (using forward difference)\n",
    "\\[\n",
    "v_1 - v_0 = 0\n",
    "\\]\n",
    "So the equation at $x_1$ becomes:\n",
    "\\[\n",
    "\\frac{v_{i+1} - 2v_i + v_i}{h^2} = 0\n",
    "\\]\n",
    "and hence the equations become:\n",
    "\\[\n",
    "\\frac{1}{h^2} \n",
    "\\begin{bmatrix}\n",
    "-1 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_{N-1} \\\\ v_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\subsection{Extending to 2D}\n",
    "\\[\n",
    "u_{xx} + u_{yy} = 0, 0 \\leq x, y \\leq 1\n",
    "\\]\n",
    "Discretise both $u_{xx}$ and $u_{yy}$ separately\n",
    "\\[\n",
    "\\frac{v_{i+1, j} - 2v_{i,j} + v_{i-1,j}}{h^2} + \\frac{v_{i, j+1} - 2 v_{i,j} + v_{i,j-1}}{h^2} = 0\n",
    "\\]\n",
    "for $i = 1, \\dots, N$, $j = 1, \\dots N$.\n",
    "\n",
    "Assuming lexicographical ordering where $i$ advances more quickly than $j$, gives the following equation (with homogenous Dirichlet bcs)\n",
    "\\[\n",
    "\\frac{1}{h^2}\n",
    "\\begin{bmatrix}\n",
    "-4 & 1 & 0 & 0 & \\dots & 1 & 0 & 0 & \\dots & 0 \\\\\n",
    "1 & -4 & 1 & 0 & \\dots & 0 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & -4 & 1 & \\dots & 0 & 0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\ddots &   \n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\subsubsection{Kronecker product}\n",
    "This matrix $L_{2D}$ is actually the sum of two terms, one associated with the discretisation of $u_{xx}$ given $L_x$ and the other associated with the discretisation of $u_{yy}$ given by $L_y$. \n",
    "\n",
    "We can use the Kronecker (tensor) product to construct $L_{2D}$.\n",
    "\\[\n",
    "L_{2D} = (I_y \\otimes L_x) + (L_y \\otimes I_x)\n",
    "\\]\n",
    "where the Kronecker product $A \\otimes B$ is given by the block matrix equations\n",
    "\\[\n",
    "A \\otimes B = \n",
    "\\begin{bmatrix}\n",
    "a_{11}B & \\dots & a_{1n}B \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}B & \\dots & a_{mn}B \\\\ \n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Each set of equations are a set of linear equations of the form:\n",
    "\\[\n",
    "Au = b\n",
    "\\]\n",
    "can find the solution $u$ via any direct linear solver. Most basic method is Gaussian elimination, many others exist based on different factorisations of the matrix A: lower-upper (LU) decompositions, QR decomposition, Cholesky decomposition\n",
    "\n",
    "Dense matrices: scipy.linalg.solve\n",
    "\n",
    "Sparse matrices: scipy.sparse.spsolve\n",
    "\n",
    "\\section{Exercises}\n",
    "\\begin{minted}[breaklines]{python}\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#Laplacian matrix needed for calculations, created with scipy.sparse\n",
    "def construct_laplace_matrix_1d(N, h):\n",
    "    e = np.ones(N)\n",
    "    diagonals = [e, -2*e, e]\n",
    "    offsets = [-1, 0, 1]\n",
    "    L = scipy.sparse.spdiags(diagonals, offsets, N, N) / h**2\n",
    "    return L\n",
    "\n",
    "#2D Laplace matrix created with Kronecker's product\n",
    "#Identity matrix generated with scipy.sparse.eye\n",
    "def construct_laplace_matrix_2d(N, h):\n",
    "    L = construct_laplace_matrix_1d(N, h)\n",
    "    I = scipy.sparse.eye(N)\n",
    "    return scipy.sparse.kron(L, I) + scipy.sparse.kron(I, L)\n",
    "\n",
    "#N points and h is the interval between points\n",
    "#However,the end points are not included in the Laplacian matrix\n",
    "#np.linalg.solve used for solving, scipy.sparse converted to ndarray with .toarray()\n",
    "def solve_poisson_dirichelet():\n",
    "    N = 100\n",
    "    x = np.linspace(0, 1, N)\n",
    "    h = x[1]-x[0]\n",
    "    x_interior = x[1:-1]\n",
    "    L = construct_laplace_matrix_1d(N-2, h)\n",
    "    b = -np.ones(N-2)\n",
    "    y_interior = np.linalg.solve(L.toarray(), b)\n",
    "    y = np.concatenate(([0], y_interior, [0]))\n",
    "    y_exact = 0.5*x - 0.5*x**2\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(x, y, label='FD')\n",
    "    plt.plot(x, y_exact, label='exact')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#Same as above, but the scipy.sparse matrix is converted to a ndarray before changing one element of the matrix\n",
    "def solve_poisson_neumann():\n",
    "    N = 100\n",
    "    x = np.linspace(0, 1, N)\n",
    "    h = x[1]-x[0]\n",
    "    x_interior = x[1:-1]\n",
    "    L = construct_laplace_matrix_1d(N-2, h).toarray()\n",
    "    L[-1,-1] /= 2.0\n",
    "    b = -np.ones(N-2)\n",
    "    y_interior = np.linalg.solve(L, b)\n",
    "    y = np.concatenate(([0], y_interior, [y_interior[-1]]))\n",
    "    y_exact = x - 0.5*x**2\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(x, y, label='FD')\n",
    "    plt.plot(x, y_exact, label='exact')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def solve_laplace_2d():\n",
    "    N = 100\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    h = x[1]-x[0]\n",
    "    #Meshgrid created with np.meshgrid (only for the interior points)\n",
    "    [xx, yy] = np.meshgrid(x[1:-1], x[1:-1])\n",
    "    #xx and yy reshaped into 1D arrays\n",
    "    x = xx.reshape(-1)\n",
    "    y = yy.reshape(-1)\n",
    "    L = construct_laplace_matrix_2d(N-2, h)\n",
    "    #np.ones_like returns an array of the same shape as x\n",
    "    b = -np.ones_like(x)\n",
    "    u = scipy.sparse.linalg.spsolve(L, b)\n",
    "    u_exact = 0.5 - 0.5*x**2\n",
    "\n",
    "    for k in range(1,100,2):\n",
    "        u_exact -= 16.0/np.pi**3*np.sin(k*np.pi*(1+x)/2)/\n",
    "        (k**3*np.sinh(k*np.pi))*(np.sinh(k*np.pi*(1+y)/2)+\n",
    "        np.sinh(k*np.pi*(1-y)/2))\n",
    "\n",
    "    error = np.linalg.norm(u_exact - u)/np.linalg.norm(u_exact)\n",
    "\n",
    "    print('error is {}'.format(error))\n",
    "\n",
    "    plt.clf()\n",
    "    plt.imshow(u.reshape((N-2, N-2)))\n",
    "    plt.show()\n",
    "\n",
    "def compare_dense_and_sparse():\n",
    "    Ns = (10**np.linspace(1, 3, 100)).astype('int')\n",
    "    time_sparse = np.empty_like(Ns, dtype='float64')\n",
    "    time_dense = np.empty_like(Ns, dtype='float64')\n",
    "    for i,N in enumerate(Ns):\n",
    "        x = np.linspace(0, 1, N)\n",
    "        h = x[1]-x[0]\n",
    "        x_interior = x[1:-1]\n",
    "        L = construct_laplace_matrix_1d(N-2, h).tocsr()\n",
    "        b = -np.ones(N-2)\n",
    "        L_dense = L.toarray()\n",
    "        t0 = time.perf_counter()\n",
    "        y_interior_sparse = scipy.sparse.linalg.spsolve(L, b)\n",
    "        t1 = time.perf_counter()\n",
    "        time_sparse[i] = t1-t0\n",
    "        t0 = time.perf_counter()\n",
    "        y_interior_dense = scipy.linalg.solve(L_dense, b)\n",
    "        t1 = time.perf_counter()\n",
    "        time_dense[i] = t1-t0\n",
    "\n",
    "\n",
    "\n",
    "    plt.clf()\n",
    "    plt.loglog(Ns, time_dense, label='dense')\n",
    "    plt.loglog(Ns, time_sparse, label='sparse')\n",
    "    plt.xlabel('N')\n",
    "    plt.ylabel('time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\\end{minted}\n",
    "\n",
    "\\section{Time integration of PDEs}\n",
    "\\subsection{Finite differences in space and time}\n",
    "The simples approach to numerical solving of PDE is finite difference discretization in both space and time (if it's time-dependent).\n",
    "\n",
    "Consider the heat or diffusion equation in 1D:\n",
    "\\[\n",
    "u_t = u_{xx}, -1 < x < 1\n",
    "\\]\n",
    "with initial conditions $u(x, 0) = u_0(x)$ and boundary conditions $u(-1, t) = u(1, t) = 0$\n",
    "\n",
    "Set up a regular grid with $k$=time step, $h=$ spatial step\n",
    "\\[\n",
    "v_j^n \\approx u(x, t)\n",
    "\\]\n",
    "A simple finite difference formula:\n",
    "\\[\n",
    "\\frac{v_j^{n+1} - v_j^n}{k} = \\frac{v_{j+1}^n - 2v_j^n + v_{j-1}^n}{h^2}\n",
    "\\]\n",
    "This can be rearranged to a linear algebra expression\n",
    "\\[\n",
    "\\begin{bmatrix}\n",
    "v_1^{n+1} \\\\\n",
    "v_2^{n+1} \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1}^{n+1} \\\\\n",
    "v_N^{n+1} \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "v_1^n \\\\\n",
    "v_2^n \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1}^n \\\\\n",
    "v_N^n \\\\\n",
    "\\end{bmatrix}\n",
    "+ \n",
    "\\frac{k}{h^2}\n",
    "\\begin{bmatrix}\n",
    "-2 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2^n \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1}^n \\\\\n",
    "v_N^n \\+\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "or \n",
    "\\[\n",
    "v^{n+1} = v^n + kLv^n\n",
    "\\]\n",
    "\\[\n",
    "v^{n+1} = (I + kL)v^n\n",
    "\\]\n",
    "Discretize in space only, to get a system of ODEs. This reduces the problem to one we've already looked at: numerical solution of ODEs.\n",
    "\n",
    "For the heat/diffusion equation, we get:\n",
    "\\[\n",
    "\\frac{d}{dt}v_j = \\frac{v_{j-1} - 2v_j + v_{j+1}}{h^2}\n",
    "\\]\n",
    "where $v_j(t)$ is a continuous function in time and $v_j(t) \\approx u(x_j, t)$. \n",
    "\n",
    "Using linear algebra\n",
    "\\[\n",
    "\\frac{d}{dt}\n",
    "\\begin{bmatrix}\n",
    "v_1(t) \\\\\n",
    "v_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1}(t) \\\\\n",
    "v_N(t) \\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\frac{1}{h^2}\n",
    "\\begin{bmatrix}\n",
    "-2 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & -2 & 1 & 0 & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 1 & -2 & 1 \\\\\n",
    "0 & 0 & 0 & 1 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_1(t) \\\\\n",
    "v_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1}(t) \\\\\n",
    "v_N(t) \\\\\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "or using $v(t)$ as $N$-vector\n",
    "\\[\n",
    "\\frac{d}{dt}v(t) = Lv(t)\n",
    "\\]\n",
    "If we then discretize time with forward Euler we get:\n",
    "\\[\n",
    "v^{n+1} = v^n + kLv^n\n",
    "\\]\n",
    "\\[\n",
    "v^{n+1} = (I+kL)v^n\n",
    "\\]\n",
    "where $v^{n+1}$ represents the discrete vector of solution at time $t_{n+1}$:\n",
    "\\[\n",
    "v^{n+1} \\approx v(t_{n+1})\n",
    "\\]\n",
    "This is the same as the earlier fully discrete equations. \n",
    "\n",
    "A fourth-order problem (biharmonic equation)\n",
    "\\[\n",
    "u_t = -u_{xxxx}\n",
    "\\]\n",
    "How to discretize? Think $(u_{xx})_{xx}$ this leads, eventually to\n",
    "\\[\n",
    "v_j^{n+1} = v_j^n - \\frac{k}{h^4}(v_{j-2}^n - 4v_{j-1}^n + 6v_j^n - 4v_{j+1}^n + v_{j+2}^n)\n",
    "\\]\n",
    "or\n",
    "\\[\n",
    "\\frac{d}{dt}v_j = \\frac{1}{h^4}(v_{j-2} - 4v_{j-1} + 6v_j - 4v_{j+1} + v_{j+2})\n",
    "\\]\n",
    "\\[\n",
    "\\frac{d}{dt}v = -Hv\n",
    "\\]\n",
    "Note that if you already have the FD approximation of $L \\approx u_{xx}$, then you can construct this as $H = LL$\n",
    "\n",
    "\\subsection{von Neumann stability analysis}\n",
    "One approach is von Neumann analysis of the finite difference formula, also known as discrete Fourier analysis. Suppose that we have perioidic boundary conditions and that at step $n$ we have a complec sine wave\n",
    "\\[\n",
    "v_j^n = \\exp(i \\xi x_j) = \\exp(i \\xi h)\n",
    "\\]\n",
    "for some wave number $\\xi$. Higher $\\xi$ is more oscillatory.\n",
    "\n",
    "We will analyse whether this wave grows in apmlitude or decays (for each $\\xi$). For stabiloty, we want all waves to decay. \n",
    "\n",
    "For the biharmonic diffusion equation, we substitute this wave into the finite difference scheme above, and factor out $\\exp(i \\xi h)$ to get\n",
    "\\[\n",
    "v_j^{n+1} = g(\\xi)v_j^n\n",
    "\\]\n",
    "with the amplification factor\n",
    "\\[\n",
    "g(\\xi) = 1 - \\frac{16k}{h^4}\\sin(\\xi g/2)\n",
    "\\]\n",
    "A mode will blow up if $|g(\\xi)| > 1$. Thus for stability we want to ensure $|g(\\xi)| \\leq 1$, i.e.\n",
    "\\[\n",
    "1 - 16k/h^4 \\geq -1\n",
    "\\]\n",
    "or\n",
    "\\[\n",
    "k \\leq h^4/8\n",
    "\\]\n",
    "For $h = 0.025$, this gives $k \\leq 4.883e-08$, and confirms that this finite difference formula is not really practical. \n",
    "\n",
    "\\subsection{Implicit methods for PDEs}\n",
    "Apply implicit (A stable) methods to the semidiscrete method-of-lines form. For example, let's look at the heat equation $u_t = \\Delta u$. If we apply backward Euler:\n",
    "\\[\n",
    "\\frac{v^{n+1} - v^n}{k} = Lv^{n+1}\n",
    "\\]\n",
    "\\[\n",
    "(I-kL)v^{n+1} = v^n\n",
    "\\]\n",
    "\\[\n",
    "Av^{n+1} = v^n\n",
    "\\]\n",
    "\n",
    "\\subsection{IMEX discretization}\n",
    "IMEX (implicit/explicit) discretizations can be used for stiff equations that contain non-linear terms, e.g.\n",
    "\\[\n",
    "u_t(t) = u_{xxxx}(t) + u^2\n",
    "\\]\n",
    "We treat the linear stiff terms implicitly and the nonlinear terms explicitly\n",
    "\\[\n",
    "\\frac{v^{n+1} - v^n}{k} = -Hv^{n+1} + v^n \\circ v^n\n",
    "\\]\n",
    "\n",
    "\\section{Exercises}\n",
    "a) Solve for $u(x, t)$ the 1D Fisher equation given below over $0 \\leq t \\leq 42$ and $0 \\leq x \\leq 20$, using an explicit Euler time-stepping scheme. Measure the speed of the expected travelling wave solution\n",
    "\\[\n",
    "u_t = u_{xx} + u(1-u)\n",
    "\\]\n",
    "\\[\n",
    "u(x, 0) = \\begin{cases}\n",
    "1 & x \\leq 3 \\\\\n",
    "0 & \n",
    "\\end{cases}\n",
    "\\]\n",
    "\\[\n",
    "u(0,t) = 1\n",
    "\\]\n",
    "\\[\n",
    "u(42, t) = 0\n",
    "\\]\n",
    "\\begin{minted}[breaklines]{python}\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "#Constructing the Laplacian matrix with scipy.sparse\n",
    "def construct_laplace_matrix_1d(N, h):\n",
    "    e = np.ones(N)\n",
    "    diagonals = [e, -2*e, e]\n",
    "    offsets = [-1, 0, 1]\n",
    "    L = scipy.sparse.spdiags(diagonals, offsets, N, N) / h**2\n",
    "    return L\n",
    "\n",
    "#Solving the Fisher equation\n",
    "def solve_fisher():\n",
    "    eps = 1.0\n",
    "    h = 0.05\n",
    "    k = 0.4 * h**2 / eps\n",
    "    print(k)\n",
    "    Tf = 42.0\n",
    "    \n",
    "    #generate x\n",
    "    x = np.arange(0+h, 20-h, h)\n",
    "\n",
    "    N = len(x)\n",
    "    L = construct_laplace_matrix_1d(N, h)\n",
    "    #bc are 1 and zero otherwise\n",
    "    bc = np.concatenate(([1], np.zeros(N-1)))/h**2\n",
    "    \n",
    "    #create array u where the first elements are equal to 1 and zero otherwise\n",
    "    u = (x<3).astype('float64')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ln, = plt.plot(x, u)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    #Update the function\n",
    "    def update(frame):\n",
    "        for i in range(10):\n",
    "            u_new = u + k*( eps*(L*u + bc) + (u-u**2) )\n",
    "            u[:] = u_new\n",
    "\n",
    "        ln.set_data(x, u)\n",
    "        ax.set_title('t = {}'.format(10*frame*k))\n",
    "        print(frame)\n",
    "        return ln,\n",
    "\n",
    "    numsteps = int(np.ceil(Tf/k)/10)\n",
    "    Tf = numsteps*k\n",
    "    ani = FuncAnimation(fig, update, frames=numsteps, interval=30, blit=False,\n",
    "            repeat=False)\n",
    "    plt.show()\n",
    "\n",
    "def solve_biharmonic():\n",
    "    h = .025\n",
    "    x = np.arange(-1+h, 1-h, h)\n",
    "    N = len(x);\n",
    "    u = (np.abs(x)<0.3).astype('float64');\n",
    "    k = 0.05*h\n",
    "\n",
    "    L = construct_laplace_matrix_1d(N, h).tolil()\n",
    "    L[0,-1] = 1.0/h**2\n",
    "    L[-1,0] = 1.0/h**2\n",
    "    H = -L.dot(L);\n",
    "\n",
    "    I = scipy.sparse.eye(N)\n",
    "    A = I - k*H\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ln, = plt.plot(x, u)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    def update(frame):\n",
    "        #unew = u + k*(H*u);     %forward euler\n",
    "        u_new = scipy.sparse.linalg.spsolve(A, u)\n",
    "        u[:] = u_new\n",
    "        ln.set_data(x, u)\n",
    "        return ln,\n",
    "\n",
    "    Tf = 0.5\n",
    "    numsteps = np.ceil(Tf / k)\n",
    "    Tf = k*numsteps\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=np.arange(0, Tf, k), interval=200, blit=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def solve_kuramoto_sivashinsky():\n",
    "    h = 0.025\n",
    "    x = np.arange(-1+h, 1-h, h)\n",
    "    N = len(x);\n",
    "    u = (np.abs(x)<0.3).astype('float64');\n",
    "    k = 0.05*h\n",
    "\n",
    "    L = construct_laplace_matrix_1d(N, h).tolil()\n",
    "\n",
    "    h = 32*np.pi/400;\n",
    "    x = np.arange(-16*np.pi+h, 16*np.pi-h, h)\n",
    "    N = len(x)\n",
    "    u = np.cos(x/16)*(1+np.sin(x/16))\n",
    "\n",
    "    k = 0.2\n",
    "    L = construct_laplace_matrix_1d(N, h).tolil()\n",
    "    L[0,-1] = 1.0/h**2\n",
    "    L[-1,0] = 1.0/h**2\n",
    "    H = L.dot(L)\n",
    "\n",
    "    e = np.ones(N)\n",
    "    D = scipy.sparse.spdiags([-e, e], [-1, 1], N, N)\n",
    "    D = 1/(2*h) * D\n",
    "\n",
    "    I = scipy.sparse.eye(N)\n",
    "\n",
    "    A = I + k*H + k*L\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ln, = plt.plot(x, u)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "    def update(frame):\n",
    "        b = u - k*(D*(u**2/2))\n",
    "        u_new = scipy.sparse.linalg.spsolve(A, b)\n",
    "        u[:] = u_new\n",
    "        ln.set_data(x, u)\n",
    "        return ln,\n",
    "\n",
    "    Tf = 3.0\n",
    "    numsteps = np.ceil(Tf / k)\n",
    "    Tf = k*numsteps\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=np.arange(0, Tf, k), interval=30, blit=True)\n",
    "    plt.show()\n",
    "\n",
    "\\end{minted}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
